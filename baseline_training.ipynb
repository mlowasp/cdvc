{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f7df62",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Training Notebook  \n",
    "## Controlled vs Uncontrolled Building Demolition Videos\n",
    "\n",
    "This notebook demonstrates a **baseline training pipeline** for the dataset using:\n",
    "\n",
    "- PyTorch\n",
    "- ResNet18 + LSTM\n",
    "- RAM-safe video loading\n",
    "- Focal Loss\n",
    "- Per-video aggregation for evaluation\n",
    "\n",
    "This baseline is intentionally **simple and conservative**, designed for **small, imbalanced video datasets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f265489",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf55870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch torchvision torchaudio opencv-python pyyaml tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57382efe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import yaml\n",
    "from model import CNNLSTMVideoClassifier\n",
    "from dataset_safe import SafeVideoDataset\n",
    "from losses import BinaryFocalLoss\n",
    "from utils import compute_metrics_from_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d5c28",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "device = cfg.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa3745",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = SafeVideoDataset(\n",
    "    root_split_dir=\"dataset/train\",\n",
    "    class_names=cfg[\"data\"][\"class_names\"],\n",
    "    num_frames=cfg[\"video\"][\"num_frames\"],\n",
    "    sampling=\"random\",\n",
    ")\n",
    "\n",
    "val_ds = SafeVideoDataset(\n",
    "    root_split_dir=\"dataset/val\",\n",
    "    class_names=cfg[\"data\"][\"class_names\"],\n",
    "    num_frames=cfg[\"video\"][\"num_frames\"],\n",
    "    sampling=\"random\",\n",
    "    fixed_seed=1234,\n",
    ")\n",
    "\n",
    "print(\"Train videos:\", len(train_ds))\n",
    "print(\"Val videos:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ba058",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6210cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CNNLSTMVideoClassifier(\n",
    "    encoder_name=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    lstm_hidden=64,\n",
    "    lstm_layers=1,\n",
    "    dropout=0.5,\n",
    "    unfreeze_layer4=False,\n",
    ").to(device)\n",
    "\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da93a00",
   "metadata": {},
   "source": [
    "## 6. Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56390e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = BinaryFocalLoss(alpha=0.75, gamma=2.0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=cfg[\"train\"][\"lr\"],\n",
    "    weight_decay=cfg[\"train\"][\"weight_decay\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1909a",
   "metadata": {},
   "source": [
    "## 7. Single Training Step (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames, labels, _ = train_ds[0]\n",
    "frames = frames.unsqueeze(0).to(device)\n",
    "labels = labels.unsqueeze(0).float().to(device)\n",
    "\n",
    "model.train()\n",
    "logits = model(frames)\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Sanity loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47cd22",
   "metadata": {},
   "source": [
    "## 8. Per-Video Aggregated Evaluation (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "probs = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(len(val_ds)):\n",
    "    clip_probs = []\n",
    "    for k in range(cfg[\"eval\"][\"clips_per_video\"]):\n",
    "        frames, label, _ = val_ds.get_clip(idx, clip_key=f\"eval_{k}\")\n",
    "        frames = frames.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logit = model(frames).clamp(-10, 10)\n",
    "            prob = torch.sigmoid(logit)[0].item()\n",
    "        clip_probs.append(prob)\n",
    "    probs.append(sum(clip_probs) / len(clip_probs))\n",
    "    labels.append(label.item())\n",
    "\n",
    "preds = torch.tensor(probs) >= cfg[\"eval\"][\"threshold\"]\n",
    "metrics = compute_metrics_from_preds(preds.long(), torch.tensor(labels))\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862889b",
   "metadata": {},
   "source": [
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook is a **baseline demonstration**, not a full training loop.\n",
    "- For full training, use `train.py`.\n",
    "- This notebook is useful for:\n",
    "  - sanity checks\n",
    "  - debugging preprocessing\n",
    "  - verifying per-video aggregation logic\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}